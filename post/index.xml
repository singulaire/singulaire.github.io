<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Tom Blau - But It Could Be Better</title>
    <link>https://singulaire.github.io/post/</link>
    <description>Recent content in Posts on Tom Blau - But It Could Be Better</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://singulaire.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Biological Optimisation as Machine Learning</title>
      <link>https://singulaire.github.io/post/biological-optimisation-as-machine-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/biological-optimisation-as-machine-learning/</guid>
      <description>The biotech field has long been in the business of trying to find new molecules or mechanisms that achieve improved function compared with what is readily available in nature. Fundamentally, this can be thought of as an optimisation problem in a very large search space. For machine learning scientists, as soon as we hear &amp;ldquo;optimisation problem&amp;rdquo; we immediately think &amp;ldquo;how can we do it better than anyone else with machine learning?</description>
    </item>
    
    <item>
      <title>Bounds on Bounds</title>
      <link>https://singulaire.github.io/post/bounds-on-bounds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/bounds-on-bounds/</guid>
      <description>This is a TODO that reminds me to eventually write a post on limitations of MI estimators and alternative solutions.</description>
    </item>
    
    <item>
      <title>Notes on Notation</title>
      <link>https://singulaire.github.io/post/notes-on-notation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/notes-on-notation/</guid>
      <description>This post is a living document of various ideas on mathematical notation that I think are interesting.
Tuple or Set? I often encounter the situation where I have a collection of mathematical objects whose ordering doesn&amp;rsquo;t matter. The default is usually to pronounce such a collection to be a tuple, and it&amp;rsquo;s common to hear people colloquially refer to any collection as a (n-)tuple even if ordering is irrelevant. For example, if you&amp;rsquo;re computing a Monte Carlo estimate of an expectation, you typically have a collection of samples and calculate their average, but how should you write this?</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Design of Experiments</title>
      <link>https://singulaire.github.io/post/rl-boed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/rl-boed/</guid>
      <description>This is a TODO that reminds me to eventually write a post on My recent paper and where we can go next.</description>
    </item>
    
    <item>
      <title>Upper and Lower Mutual Information Bounds</title>
      <link>https://singulaire.github.io/post/upper-and-lower-mutual-information-bounds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/upper-and-lower-mutual-information-bounds/</guid>
      <description>Recently Foster et al (2020) introduced a couple of tractable bounds that can be used to estimate the expected information gain (EIG) of a design policy (a mapping from past designs and observations to the next design). These include the sequential Prior Contrastive Estimate (sPCE) lower bound:
$$ \mathcal{L}_T(\pi) = \mathbb{E} \left[ \log \frac{p(h_T | \theta_0, \pi)}{\frac{1}{L+1} \sum_0^L p(h_T | \theta_l, \pi)} \right] $$
where $h_T = (d_0, y_0,\dots,d_T,y_T)$ is the experimental history at time $T$, $\pi$ is the design policy, $L$ is the number of contrastiv samples and $\theta$ parameterises the experimental model $p(y|\theta, d)$ that maps designs to a distribution over outcomes.</description>
    </item>
    
  </channel>
</rss>
