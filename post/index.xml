<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Tom Blau - But It Could Be Better</title>
    <link>https://singulaire.github.io/post/</link>
    <description>Recent content in Posts on Tom Blau - But It Could Be Better</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://singulaire.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bounds on Bounds</title>
      <link>https://singulaire.github.io/post/bounds-on-bounds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/bounds-on-bounds/</guid>
      <description>This is a TODO that reminds me to eventually write a post on limitations of MI estimators and alternative solutions.</description>
    </item>
    
    <item>
      <title>Notes on Notation</title>
      <link>https://singulaire.github.io/post/notes-on-notation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/notes-on-notation/</guid>
      <description>This post is a living document of various ideas on mathematical notation that I think are interesting.
Tuple or Set? I often encounter the situation where I have a collection of mathematical objects whose ordering doesn&amp;rsquo;t matter. The default is usually to pronounce such a collection to be a tuple, and it&amp;rsquo;s common to hear people colloquially refer to any collection as a (n-)tuple even if ordering is irrelevant. For example, if you&amp;rsquo;re computing a Monte Carlo estimate of an expectation, you typically have a collection of samples and calculate their average, but how should you write this?</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Design of Experiments</title>
      <link>https://singulaire.github.io/post/rl-boed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/rl-boed/</guid>
      <description>This is a TODO that reminds me to eventually write a post on My recent paper and where we can go next.</description>
    </item>
    
    <item>
      <title>Upper and Lower Mutual Information Bounds</title>
      <link>https://singulaire.github.io/post/upper-and-lower-mutual-information-bounds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/upper-and-lower-mutual-information-bounds/</guid>
      <description>Recently Foster et al (2020) introduced a couple of tractable bounds that can be used to estimate the expected information gain (EIG) of a design policy (a mapping from past designs and observations to the next design). These include the sequential Prior Contrastive Estimate (sPCE) lower bound:
$$ \mathcal{L}_T(\pi) = \mathbb{E} \left[ \log \frac{p(h_T | \theta_0, \pi)}{\frac{1}{L+1} \sum_0^L p(h_T | \theta_l, \pi)} \right] $$
where $h_T = (d_0, y_0,\dots,d_T,y_T)$ is the experimental history at time $T$, $\pi$ is the design policy, $L$ is the number of contrastiv samples and $\theta$ parameterises the experimental model $p(y|\theta, d)$ that maps designs to a distribution over outcomes.</description>
    </item>
    
  </channel>
</rss>
