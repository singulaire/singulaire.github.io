<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tom Blau - But It Could Be Better</title>
    <link>https://singulaire.github.io/</link>
    <description>Recent content on Tom Blau - But It Could Be Better</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://singulaire.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://singulaire.github.io/top/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/top/about/</guid>
      <description>About Me I&amp;rsquo;m a machine learning researcher interested in using reinforcement learning to do something more valuable than just generating papers. I completed my Ph.D. at the university of Sydney in 2020, completing a thesis on reinforcement learning for robotics. One of the most striking lessons from my Ph.D. was the gap between state-of-the-art RL and actual, real-world applications that provide benefit to flesh-and-blood humans. Since then I have been working on RL solutions for real problems, with a current focus on Bayesian optimal design of experiments.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://singulaire.github.io/top/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/top/publications/</guid>
      <description>Publications Below follows a list of my publications:
Optimizing Sequential Experimental Design with Deep Reinforcement Learning
Tom Blau, Edwin Bonilla, Iadine Chades, and Amir Dezfouli.
ICML, 2022
[pdf]
Learning from Demonstration without Demonstrations
Tom Blau, Philipe Morere, Gilad Francis
ICRA, 2021
[pdf]
Bayesian Curiosity for Efficient Exploration in Reinforcement Learning
Tom Blau, Lionel Ott, Fabio Ramos
ACRA, 2020
[pdf]
Improving Reinforcement Learning Pre-Training with Variational Dropout
Tom Blau, Lionel Ott, Fabio Ramos</description>
    </item>
    
    <item>
      <title>Bounds on Bounds</title>
      <link>https://singulaire.github.io/post/bounds-on-bounds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/bounds-on-bounds/</guid>
      <description>This is a TODO that reminds me to eventually write a post on limitations of MI estimators and alternative solutions.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Design of Experiments</title>
      <link>https://singulaire.github.io/post/rl-boed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/rl-boed/</guid>
      <description>This is a TODO that reminds me to eventually write a post on My recent paper and where we can go next.</description>
    </item>
    
    <item>
      <title>Upper and Lower Mutual Information Bounds</title>
      <link>https://singulaire.github.io/post/upper-and-lower-mutual-information-bounds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://singulaire.github.io/post/upper-and-lower-mutual-information-bounds/</guid>
      <description>Recently Foster et al (2020) introduced a couple of tractable bounds that can be used to estimate the expected information gain (EIG) of a design policy (a mapping from past designs and observations to the next design). These include the sequential Prior Contrastive Estimate (sPCE) lower bound:
$$ \mathcal{L}_T(\pi) = \mathbb{E} \left[ \log \frac{p(h_T | \theta_0, \pi)}{\frac{1}{L+1} \sum_0^L p(h_T | \theta_l, \pi)} \right] $$
where $h_T = (d_0, y_0,\dots,d_T,y_T)$ is the experimental history at time $T$, $\pi$ is the design policy, $L$ is the number of contrastiv samples and $\theta$ parameterises the experimental model $p(y|\theta, d)$ that maps designs to a distribution over outcomes.</description>
    </item>
    
  </channel>
</rss>
